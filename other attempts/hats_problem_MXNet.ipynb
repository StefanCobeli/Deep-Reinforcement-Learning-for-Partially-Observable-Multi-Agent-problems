{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Hats problem in keras using a MXNet backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU gpu(0) in use!\n"
     ]
    }
   ],
   "source": [
    "# Test mxnet GPU usage:\n",
    "\n",
    "import mxnet as mx \n",
    "def gpu_device(gpu_number=0):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=mx.gpu(gpu_number))\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return mx.gpu(gpu_number)\n",
    "\n",
    "if not gpu_device(0):\n",
    "    print('No GPU device found!')\n",
    "else:\n",
    "    print('GPU {0:s} in use!'.format(str(mx.gpu(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using MXNet backend\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make lists of NN components for each prisoner (This part may definetly be improoved)  \n",
    "inputs1 = []\n",
    "W = []\n",
    "Qout = []\n",
    "predict = []\n",
    "nextQ = []\n",
    "loss = []\n",
    "trainer = []\n",
    "updateModel = []\n",
    "models = []\n",
    "\n",
    "number_of_agents = 2\n",
    "for ag in range(number_of_agents):\n",
    "    \n",
    "    nb_inputs = 5\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation='relu', input_dim=number_of_agents - 1))\n",
    "    #model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(number_of_agents, activation='sigmoid'))\n",
    "    model.compile(optimizer='sgd',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ETA: 0:0:0. Percentage of winning episodes, after 30000, out of 30000 (100.00%), episodes is 0.74.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "# Set learning parameters\n",
    "e = 0.1\n",
    "num_episodes = 3 * (10 ** 4)\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "rAll = 0\n",
    "eta_per_eps = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    \n",
    "    ep_start_time = time.time()\n",
    "    #initialize hats\n",
    "    hats = [np.random.randint(number_of_agents) for i in range(number_of_agents)]\n",
    "\n",
    "    #trasform the seen hats into inputs for the network\n",
    "    states = [hats[:i] + hats[i+1:] for i in range(number_of_agents)]\n",
    "\n",
    "    #The Q-Network\n",
    "    #Take actions in for the agents\n",
    "    actions = []\n",
    "    for i in range(number_of_agents):\n",
    "        state = np.array(states[i]) / number_of_agents #normalize the input\n",
    "        predictions = models[i].predict( np.reshape(state, (-1, number_of_agents - 1)))\n",
    "        action      = predictions[0].argmax()\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(number_of_agents) #env.action_space.sample()\n",
    "        actions.append((action, predictions[0]))\n",
    "        \n",
    "        \n",
    "    validate = sum(actions[x][0] == hats[x] for x in range(number_of_agents))\n",
    "    r = 0\n",
    "    r += 1 if validate >= 1 else 0\n",
    "    \n",
    "    \n",
    "    #update the Q networks\n",
    "    for i in range(number_of_agents):\n",
    "        targetQ = [actions[i][1]]\n",
    "        targetQ[0][actions[i][0]] = r\n",
    "        targetQ = np.reshape(targetQ, (-1, number_of_agents))\n",
    "        state =  np.reshape(np.array(states[i]) / number_of_agents, (-1, number_of_agents - 1))\n",
    "        models[i].fit(state, targetQ, verbose=0)\n",
    "        \n",
    "        \n",
    "    #record rewards\n",
    "    rAll += r\n",
    "    eta_per_eps.append(time.time() - ep_start_time)\n",
    "    \n",
    "    #Update displayed information and epsilon\n",
    "    if(ep % 200 == 199):\n",
    "        episode = ep\n",
    "        winning_episode_percentage = rAll / 200.\n",
    "        training_percentage = (episode + 1) / num_episodes * 100\n",
    "        \n",
    "        sys.stdout.write(\"\\r             Percentage of winning episodes, after {0:d}, out of {1:d} ({2:.2f}%), episodes is {3:.2f}.\"\\\n",
    "              .format(episode + 1, num_episodes, training_percentage, winning_episode_percentage))\n",
    "        \n",
    "        \n",
    "        #Reduce chance of random action as we train the model.\n",
    "        e = 1./((ep/10) + 10)\n",
    "\n",
    "        #Updated accuracy\n",
    "        rList.append(rAll / 200.)\n",
    "        rAll = 0         \n",
    "        \n",
    "        #Print estimated time \n",
    "        eta_seconds = int(((sum(eta_per_eps[-30:]) / 30) * (num_episodes - episode)))\n",
    "        eta_hours   = eta_seconds // 3600\n",
    "        eta_minutes = (eta_seconds % 3600) // 60\n",
    "        eta_seconds = eta_seconds % 60\n",
    "        sys.stdout.write(\"\\r ETA: {0:0d}:{1:0d}:{2:0d}.\".format(eta_hours, eta_minutes, eta_seconds))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test the network results and plot obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7523"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(Q_nets):\n",
    "    number_of_agents = len(Q_nets)\n",
    "    Q = Q_nets\n",
    "    reward = 0\n",
    "    for ep in range(10000):\n",
    "        hats = [np.random.randint(number_of_agents) for i in range(number_of_agents)]\n",
    "        #trasform the seen hats into inputs for the network\n",
    "        states = [hats[:i] + hats[i+1:] for i in range(number_of_agents)]\n",
    "        \n",
    "        actions = []\n",
    "        for i in range(number_of_agents):\n",
    "            state = np.array(states[i]) / number_of_agents #normalize the input\n",
    "            predictions = Q[i].predict( np.reshape(state, (-1, number_of_agents - 1)))\n",
    "            action      = predictions[0].argmax()\n",
    "            actions.append((action, predictions[0]))\n",
    "            \n",
    "        #verify if the the agents solved the problem\n",
    "        #(i.e. at least one agent correctly guessed the color of its own hat)\n",
    "        validate = sum(actions[x][0] == hats[x] for x in range(number_of_agents))\n",
    "        r = 0\n",
    "        r += 1 if validate >= 1 else 0\n",
    "\n",
    "        reward += r\n",
    "    return reward / 10000.\n",
    "test_accuracy(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_accuracy = rList\n",
    "plt.figure(figsize=(8, 4), dpi= 120)\n",
    "plt.axes()\n",
    "plt.title(\"Accuracy improvement for {0:d} prisoners using Q nets\".format(number_of_agents))\n",
    "plt.plot(batch_accuracy, label = \"Accuracy\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Training percentage\")\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "\n",
    "print(\"For training\", number_of_agents, \"agents (using Q nets), for\", num_episodes, \"episodes,\")\n",
    "print(\"the accuracy started at:\", batch_accuracy[0], \"and reached:\", batch_accuracy[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mxnet-gpu]",
   "language": "python",
   "name": "conda-env-mxnet-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
